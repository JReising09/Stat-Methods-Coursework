---
title: "Stat Methods I - Homework 3"
author: "Justin Reising"
date: "October 11, 2018"
output: pdf_document

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(car)
library(lawstat)
library(knitr)
library(ggplot2)
```

### 1.5) When asked to state the simple linear regression model, a student wrote it as follows:(Do you agree?)
$$ E[Y_{i}] = \beta_{0} + \beta_{1}X_{i} + \epsilon_{i}$$

> I disagree with the student's response. Note that the simple linear regression model has a slight difference and is as follows: $Y_{i} = \beta_{0} + \beta_{1}X_{i} + \epsilon_{i}$. Note that the response variable $Y_{i}$ has a probability distribution whose mean is $E[Y_{i}] = \beta_{0} + \beta_{1}X_{i}$ which is known as the deterministic component. Also note that $\epsilon_{i}$ falls off for the $E[Y_{i}]$ since $E[\epsilon_{i}] = 0$. 

### 1.7) In a simulation excercise, regression model (1.1) applies with $\beta_{0} = 100$, $\beta_{1} = 20$, and $\sigma^{2} = 25$. An observation on $Y$ will be made for $X=5$.

**(a)** Can you state the exact probability that $Y$ will fall between 195 and 205. Explain.

> Well, based on the the following question, No. But we need the assumption to be satisfied that each $\epsilon_{i} ~ N(0,\sigma^2)$, which we can formally test with the Shapiro-Wilk Test.

**(b)** If the normal error regression (1.24) is applicable, can you now state the exact probability that $Y$ will fall between 195 and 205? If so, state it. 

> With the assumption that each $\epsilon_{i} ~ N(0,\sigma^2)$ as with the normal error regression, then we can find the probability $195 \leq Y \leq 205$ such that with $E[Y] = 100 + 20(5) = 200$ and $\sigma^2 = 25$ by calculating normal probabilities:

```{r}
pnorm(205,mean = 200,sd = 5) - pnorm(195, mean = 200, sd = 5)
```

### 1.11) The regression function relating production output by an employee after taking a training program ($Y$) to the production output before the training program ($X$) is $E[Y]=20 + .95X$, where $X$ ranges from 40 to 100. An observer concludes that the training program does not raise production output on the average because $\beta_{1}$ is not greater than 1. Comment.

> Well, there is no context for the unit of measure for "production output" as to whether it is some number of units of product being produced or a percentage, etc. To me, a simple linear regression model is not ideal for this type of investigation. The reason being, take two employees, A adn B, where their prior production levels are 40 and 100 respectively. If this unit of measure is percentage, then employee B has no room for improvement and the model will indicate the training had no effect. For employee B however, there is 60 percent of improvement possible and any type of additional training is likely to increase production. We also must consider variation of production between time frames (days,weeks,etc.). How do we know if the observed difference in production is not just a result from likely variations? This type of investigation requires an experimental design in my opinion. All of that aside, after introducing a training program, I would expect those with lower prior production to have higher increases in production after the training than those with higher prior production, meaning that $\beta_{1}$ should have a negative slope. Personally, I think skill and ability, when measured appropriately, is nonlinear in that the better you are at something, the harder you have to work to see improvement. 

### 1.16) Evaluate the following statement: "For the least squares method to be fully valid, it is required that the distribution of $Y$ be normal."

> I'm not sure what it means to be "fully" valid as oppose to "partially" or "mostly" valid but it does not seem to me that the requirement of the distribution of $Y$ to be normal needs to be satisfied, just as long as the Predictor variables are linearly correlated and it is unbiased. The consequence would be that we would obtain non-normal parameters.

### 1.17) A person states that $b_{0}$ and $b_{1}$ in the fitted regression function (1.13) can be estimated by the method of least squares. Comment.

> Note that the primary objective of the least squares method is to find values of $b_{0}$ and $b_{1}$ that minimize $\sum_{i=1}^{n} (Y_{i} - \beta_{0} - \beta_{1}X_{i})^{2}$. These values are calculated from the sampled data and are unbiased estimators of $\beta_{0}$ and $\beta_{1}$.

### 1.18) According to (1.17) $\sum e_{i} = 0$ when regression model (1.1) is fitted to a set of $n$ cases by the method of least squares. Is it also true that $\sum \epsilon_{i} = 0$? Comment.

> Note that each $e_{i}$ is an unbiased estimator of $\epsilon_{i}$ and the assumption that that each $\epsilon_{i} ~ N(0,\sigma^{2})$ so we have the $\sum e_{i} = 0 \implies \sum \epsilon_{i} = 0$.

\pagebreak

### 1.19)  Grade Point Average. The director of admission of a small college selected 120 students at random from the new freshman class in a study to determine whether a student's grade point average (GPA) at the end of the freshman year ($Y$) can be predicted from the from the ACT score ($X$). The results of the study follow. Assume that first-order regression model (1.1) is appropriate.


```{r, include = F, echo=FALSE}
my.datafile <- tempfile()
cat(file=my.datafile, "
 GPA ACT
 3.897 21
 3.885 14
 3.778 28
 2.540 22
 3.028 21
 3.865 31
 2.962 32
 3.961 27
 0.500 29
 3.178 26
 3.310 24
 3.538 30
 3.083 24
 3.013 24
 3.245 33
 2.963 27
 3.522 25
 3.013 31
 2.947 25
 2.118 20
 2.563 24
 3.357 21
 3.731 28
 3.925 27
 3.556 28
 3.101 26
 2.420 28
 2.579 22
 3.871 26
 3.060 21
 3.927 25
 2.375 16
 2.929 28
 3.375 26
 2.857 22
 3.072 24
 3.381 21
 3.290 30
 3.549 27
 3.646 26
 2.978 26
 2.654 30
 2.540 24
 2.250 26
 2.069 29
 2.617 24
 2.183 31
 2.000 15
 2.952 19
 3.806 18
 2.871 27
 3.352 16
 3.305 27
 2.952 26
 3.547 24
 3.691 30
 3.160 21
 2.194 20
 3.323 30
 3.936 29
 2.922 25
 2.716 23
 3.370 25
 3.606 23
 2.642 30
 2.452 21
 2.655 24
 3.714 32
 1.806 18
 3.516 23
 3.039 20
 2.966 23
 2.482 18
 2.700 18
 3.920 29
 2.834 20
 3.222 23
 3.084 26
 4.000 28
 3.511 34
 3.323 20
 3.072 20
 2.079 26
 3.875 32
 3.208 25
 2.920 27
 3.345 27
 3.956 29
 3.808 19
 2.506 21
 3.886 24
 2.183 27
 3.429 25
 3.024 18
 3.750 29
 3.833 24
 3.113 27
 2.875 21
 2.747 19
 2.311 18
 1.841 25
 1.583 18
 2.879 20
 3.591 32
 2.914 24
 3.716 35
 2.800 25
 3.621 28
 3.792 28
 2.867 25
 3.419 22
 3.600 30
 2.394 20
 2.286 20
 1.486 31
 3.885 20
 3.800 29
 3.914 28
 1.860 16
 2.948 28
", sep=" ")
options(scipen=999) # suppressing scientific notation

    
grade<-read.table(my.datafile,header=T)

attach(grade)


names(grade)
```

```{r, echo = F}
kable(head(grade), caption = "Grade Point Average (Header)")
```

**(a)** Obtain the least squares estimates for $\beta_{0}$ and $\beta_{1}$, and state the estimated regression function.

```{r, include = T}
b1<- cor(GPA,ACT)*(sd(GPA)/sd(ACT))
b0<- mean(GPA)-(b1*mean(ACT))
b0
b1
```
$$ E[Y_{i}] = 2.114 + 0.039 X_{i}$$


**(b)** Plot the estimated regression function and the data. Does the estimated regression function appear to fit the data well?

```{r, echo = F, out.width='60%', fig.align='center'}
ggplot(data = data.frame(ACT,GPA), aes(x = ACT, y = GPA)) + 
  geom_point(color='blue') +
  geom_smooth(method = "lm", se = T, color = "red") +
  ggtitle("Estimated Regression Function Plot")
```

> Note that there is some wide variation in the distribution of the residuals from the regression line. One notable observation is a student that scored what looks to be a 29 on the ACT ended up with 0.5 GPA which may be a considerable case for removal of an outlier. Overall it seems to be a decent fit for the data although it does seem quite scattered.

**(c)** Obtain a point estimate of the mean freshman GPA for the students with ACT scores $X=30$.

$$ \hat{Y} = 2.114 + 0.039(30) = 3.284$$

**(d)** What is the point estimate of the change in the mean response when entrance test score increases by one point?

$$ 0.039 $$


### 1.22) Plastic Hardness: Refer to problem 1.3 and 1.14. Sixteen batches of plastic were made and from each batch one test item was molded. Each item was randomly assigned to one of the four predetermined time levels, and the hardness was measured after the assigned elapsed time. The results are as shown below, $X$ is the elapsed time in hours, and $Y$ is hardness in Brinell units. Assume that first-orderregression model (1.1) is appropriate.

```{r, include = F, echo=F}
my.datafile2 <- tempfile()
cat(file=my.datafile2, "
Hardness Time
 199.0  16.0
 205.0  16.0
 196.0  16.0
 200.0  16.0
 218.0  24.0
 220.0  24.0
 215.0  24.0
 223.0  24.0
 237.0  32.0
 234.0  32.0
 235.0  32.0
 230.0  32.0
 250.0  40.0
 248.0  40.0
 253.0  40.0
 246.0  40.0

", sep=" ")
options(scipen=999) # suppressing scientific notation

    
plastic<-read.table(my.datafile2,header=T)

attach(plastic)

names(plastic)

```

```{r, echo = F}
kable(head(plastic), caption = "Plastic Hardness (Header)")
```


**(a)** Obtain the estimated regression function. Plot the estimated regression function and the data. Does the linear regression function appear to give a good fit here?

```{r, include = T}
b1<- cor(Hardness,Time)*(sd(Time)/sd(Hardness))
b0<- mean(Hardness)-(b1*mean(Time))
b0
b1
```

$$ E[Y_{i}] = 212.1693 + 0.4783 X_{i} $$

**(b)** Obtain a point estimate of the mean hardnes when $X=40$.

$$ \hat{Y} = 212.1693 + 0.4783(40) = 231.3$$

**(c)** Obtain a point estimate of the change in the mean hardnes when $X$ increases by 1 hour.

\pagebreak

### 1.23) Refer to Grade Point Average. Problem 1.19.

**(b)** Estimate $\sigma^2$ and $\sigma$. In what units is $\sigma$ expressed?

```{r}
grade.reg<-lm(GPA~ACT)
SSE_grade<- sum((grade.reg$residuals)^2)
MSE_grade<- SSE_grade/(length(ACT)-2)
MSE_grade
sqrt(MSE_grade)
```
$$\sigma^2 = 0.388  \ \ \sigma = 0.623$$

> Sigma is measured in units of GPA.

### 1.26) Refer to Plastic Hardness Problem 1.22

**(b)** Estimate $\sigma^2$ and $\\sigma$. In what units is $\sigma$ expressed?

```{r}
plastic.reg<-lm(Hardness~Time)
SSE_plastic<- sum((plastic.reg$residuals)^2)
MSE_plastic<- SSE_plastic/(length(Time)-2)
MSE_plastic
sqrt(MSE_plastic)
```
$$\sigma^2 = 10.459  \ \ \sigma = 3.234$$

> Sigma is measured in Brinell units.

### 1.30) Refer to regression model (1.1). What is the implication for the regression function if $\beta_{1}=0$ so that the model is $Y_{i} = \beta_{0}+\epsilon_{i}$ How would the regression function plot on a graph?

> The regression function would be constant at $\beta_{0}$

\pagebreak

### 1.33) (Calculus needed) Refer to the regression model $Y_{i} = \beta_{0} + \epsilon_{i}$ in Exercise 1.30. Derive the least squares estimator of $\beta_{0}$ for this model.

$$ Q = \sum_{i=1}^{n} ( Y_{i} - \beta_{0} ) ^{2} $$

$$ 
\begin{aligned}
\frac{\partial Q}{\partial \beta_{0}} = -2 \sum_{i=1}^{n} ( Y_{i} - \beta_{0} ) &= 0 \\
-2 ( \sum_{i=1}^{n} Y_{i} - n \beta_{0} ) &= 0 \\
\sum_{i=1}^{n} Y_{i} - n \beta_{0}  &= 0 \\
\frac{1}{n} \sum_{i=1}^{n} Y_{i} &= \beta_{0} \\
\bar{Y} &= \beta_{0}
\end{aligned}
$$

### 1.40) In fitting regression model (1.1), it was found that observation $Y_{i}$ fell directly on the fitted regression line (i.e $Y_{i} = \hat{Y}_{i}$). If this case were deleted, would the least squares regression line fitted to the remaining $n-1$ cases be changed? [$\textit{HINT}$ What is the contribution of case $i$ to the least squares criterion $Q$ in (1.8)?]

> This would be the equivalent of adding a value equal to the average of a sampling distribution to the sample over and over again. Although $n$ increases, the shape, center, and spread of the distribution remains the same.

\pagebreak

### 2.1) A student working on a summer internship in the economic research department of a large corporation studied the relation between sales of a product ($Y$, in millions of dollars) and population ($X$, in million persons) in the firm's 50 marketing districts. The normal error regression model (2.1) was employed. The student first wished to test whether or not a linear association between $Y$ and $X$ existed. The student accessed a simple linear regression program and obtained the information in Figure 1 above on the regression coefficients:

![Regression Coefficients 2.1](C:\Users\jreis\Dropbox\ETSU Graduate Work\Fall 2018 Graduate Studies\Stat Methods 1\Problem 2.1 Figure)

**(a)** The student concluded from these results that there is a linear association between $Y$ and $X$. Is the conclusion warranted? What is the implied level of significance?

> The implied level of significance is 95% based on the confidence limits in the figure above. Since the estimated value of of the Slope ($\beta_{0}$) $\neq 0$, then I would have to agree the there is a linear association between $Y$ and $X$. Whiele I cannot formally assess the claim, it appears to be a warranted conclusion.


### 2.2) In a test of the alternatives $H_{0}: \beta_{1} \leq 0$ versus $H_{1}: \beta_{1} > 0$, an analyst concluded $H_{0}$. Does this conclusion imply that there is no linear association between $X$ and $Y$? Explain.

> As the analyst fails to reject the null hypothesis $H_{0}$, we haven't really answered the question of whether or not $X$ and $Y$ are linearly associated. We have however confirmed the claim that $X$ and $Y$ are either negatively correlated or not at all. IF we look at the confidence limits for a 95% or 99% confidence levels the nwe may be able to ascertain if 0 is in the intervals or not, which may give us an indication as to whether or not $X$ and $Y$ are linearly associated. 

\pagebreak

### 2.3) A member of a student team is playing an interactive marketing game recieved the following computer output when studying the relation between advertising expenditures ($X$) and sales ($Y$) for one output of the team's products:

$$
\text{Estimated regression equation:} \ \  \hat{Y} = 350.7 -0.18X $$
$$ \text{Two-Sided p-value for est. slope:} \ 0.91 
$$

### The student states: "The message I get here is that the more we spend on advertising this product, the fewer units we sell!!" Comment.

> At first glance of the estimated regression equation, it is true that there is a negative slope, however upon further investigation of the p-value below, we can see that there is not a significance level worth testing that would allow for a p-value that high to conclude the estimated slope in the estimated regression equation.

### 2.4) Refer to the GPA problem 1.19.

**(a)** Obtain a 99 percent confidence interval for $\beta_{1}$. Interpret you confidence interval. Does it include 0? Why might the director admissions be interested in whether the confidence interval includes 0?

```{r, include=T}
confint(grade.reg,"ACT",level=.99)
```

> As you can see above, with 99% confidence the true change in GPA per ACT score point is between 0.0054 and 0.0723. Note that while the lower limit for the 99% Confidence Interval is close to 0, 0 is not in the confidence interval. The director of the admissions would be interested in this because if 0 were in this interval, then it would indicate that there could be no change in GPA per ACT unit and these two variables may not be correlated.

**(b)** Test, using a test statstic $t^{*}$, whether or not the linear association exists between student's ACT score ($X$) and GPA ($Y$). Use a significance level of 0.01. State the alternatives, decision rule, and conclusion.\

$$ H_{0}: \beta_{1} = 0 \ \ \ H_{1}: \beta_{1} \neq 0 $$

```{r, include = T}
alpha <- 0.01
t_star<- (grade.reg$coefficients[2])/(sqrt(MSE_grade/sum((ACT-mean(ACT))^2)))
t_star
t_star > qt(1-alpha/2, length(ACT)-2)
```

> Note that since $t^{*} > t_{(1-\frac{\alpha}{2}, n-2)}$ the rejection rule for the critical value holds as we reject the null hypothesis $H_{0}$. As we saw in the 99% confidence interval, we conclude that there is a linear association between ACT scores and GPA.

\pagebreak 

**(c)** What is the p-value of your test in part (b)? How does it support the conclusion reached in part (b)?

```{r}
pval_grade<- 2*pt(-t_star, length(ACT)-2)
pval_grade
```

> We obtain a p-value of 0.00292 which is less than our significance level of 0.01 and confirms our conclusion in part (b). This p value indicates that due to random variation, we expect to see approximately 3/1000 samples in which no correlation between $X$ and $Y$ would be evident. So within our predetermined accepted significance level, this result strongly supports the conlcusion in part (b).

### 2.7) Refer to Plastic Hardness Problem 1.22.

**(a)** Estimate the change in the mean hardness when the elapsed time increases by one hour. Use a 99% confidence interval. Interpet your interval estimate.


**(b)** The plastic manufacturer has stated that the mean hardness should increase by 2 Brinell units per hour. Conduct a two-sided test to decide whether this standard is being satisfied; use $\alpha = 0.1$. State tha alternatives, decision rule, and conclusion. What is the P-value of the test?

$$ H_{0}: \beta_{1} = 2 \ \ \ H_{1}: \beta_{1} \neq 2 $$

```{r, include=T}
alpha <- 0.01
t_star<- (plastic.reg$coefficients[2]-2)/(sqrt(MSE_plastic/sum((Time-mean(Time))^2)))
t_star > qt(1-alpha/2, length(Time)-2)
pval_plastic_2<- 2*pt(-t_star, length(Time)-2)
pval_plastic_2
```

> Note that since $t^{*} \ngtr t_{(1-\frac{\alpha}{2}, n-2)}$ the rejection rule for the critical value fails so we fail to reject the null hypothesis $H_{0}$. With 99% confidence, we conclude that the manufacturer's claim is consistent. The P-value, 0.0709 for this hypothesis test is well over $\alpha = 0.01$, our predetermined level of acceptance.


### 2.9) Refer to figure 2.2. A student noting that $s \left\{ b_{1} \right\}$ is furnished in the printout, asks why $s \left\{ \hat{Y_{h}} \right\}$ is not also given. Discuss.

> It is furnished indirectly as the square root of the MSE for the Error since the standard deviation of a predicted value on the regression line is the standard deviation of the error.

\pagebreak

### 2.13) Refer ot Grade Point Average Problem 1.19

**(a)** Obtain a 95% interval estimate of the mean Freshman GPA for students whose ACT test score is 28. Interpret you confidence interval.

```{r, include=T}
pred_act<-data.frame(ACT = 28)
predict(grade.reg, pred_act, interval = "confidence", level = 0.95)
```

> With 95% confidence, the estimated true mean Freshan GPA for students whose ACT score is 28 is approximately between 3.06 and 3.34.

**(b)** Mary Jones obtained a score of 28 on the entrance test. Predict her freshman GPA using a 95% prediction interval. Interpret your prediction interval.


```{r, include = T}
predict(grade.reg, pred_act, interval = "prediction", level = 0.95)
```

> With 95% probability, the estimated true mean Freshman GPA for students whose observed ACT score is 28 is approximately between 1.96 and 4.44. (Assuming the max GPA is 4.0, then the prediction interval would be 1.96 to 4.0)

**(c)** Is the prediction interval in part (b) wider than the confidence interval in part (a)? Should it be?

> The prediction interval is wider than the confidence interval because the predicted value accounts for added variability from a new sample estimate.


### 2.16) Refer to Plastic Hardness Problem 1.22.

**(a)** Obtain a 98% confidence interval for the mean hardness molded items with an elapsed time of 30 hourse. Interpret your confidence interval.

```{r, include=T}
pred_time<-data.frame(Time = 30)
predict(plastic.reg, pred_time, interval = "confidence", level = 0.98)
```

> With 98% confidence the true mean hardness for 30 hours time elapsed is between 227.46 and 231.81 Brinell units.

**(b)** Obtain a 98% prediction interval for the hardness of a newly molded test item with an elapsed time of 30 hourse. Interpret your confidence interval.

```{r, include = T}
predict(plastic.reg, pred_time, interval = "prediction", level = 0.98)
```

\pagebreak

### 2.17) An analyst fitted a normal error regression model (2.1) and conducted an $F$ test of $\beta_{1}=0$ vs $\beta_{1} \neq 0$. The P-Value of the test was 0.033, and the analyst concluded $H_{0}: \beta_{1} \neq 0$. Was the $\alpha$ level used by the analyst greater than or smaller than 0.033? If the $\alpha$ level had been 0.01, what would have been the appropriate conclusion?

> As the analyst failed to reject the null hypothesis, the $\alpha$ level used had to be greater than 0.033 (presumably 0.05). If the analyst had ised an $\alpha$ level of 0.01, they would have rejected the null hypothesis.

### 2.18) For conducting statistical tests concerning the parameter $\beta_{1}$, why is the t-test more versatile than the $F$ test?

> The t-test is more versatile since it can be used to test for one-sided and two-sided alternatives where the F-test is a one sided test since the F distribution is just the square of a t distribution.

### 2.26 Refer to Plastic Hardness Problem 1.22

**(a)** Set up the ANOVA table.

```{r, include = T}
anova_plastic<- anova(plastic.reg)
anova_plastic
```

**(b)** Test by means of an F test whether or not there is a linear association between the hardness of the plastic and the elapsed time. Use $\alpha = 0.01$. State the alternatives, decision rule, and conclusion.

$$
H_{0}: \beta_{1} = 0 \ \ \ H_{1}: \beta_{1} \neq 0
$$
```{r include = T}
f_star<- anova_plastic$`F value`[1]
f_star > qf(.99,1,length(Time)-2)
```

> Note that using the critical value approach, since $F^{*} > F_{(1-\alpha, 1, n-2)},$ we reject the null hypothesis, $H_{0}$ and conclude that there is a linear association between the hardness of the plastic and the elapsed time.

\pagebreak

**(d)** Calculate $R^{2}$ and $r$ and interpret the values.

```{r, include=T}
r<- cor(Time, Hardness)
r
r^2
```

> 'r' is the sample correlation between 'Time' and 'Hardness' which we can see is highly correlated with a value of 0.986. With $R^{2} = 0.9731$, we interpret this as 97.31% of the variation in Hardness is explained by the linear relationship with time elapsed. 