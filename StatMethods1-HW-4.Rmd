---
title: "Stat Methods I - Homework 4"
author: "Justin Reising"
date: "October 29, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(car)
library(lindia)
library(gridExtra)
library(GGally)
library(lawstat)
library(knitr)
library(ggplot2)
```

### 3.3) Refer to Grade Point Average Problem 1.19 from HW 3. 

```{r GPA data, include=FALSE, echo=F}
my.datafile <- tempfile()
cat(file=my.datafile, "
 GPA ACT
 3.897 21
 3.885 14
 3.778 28
 2.540 22
 3.028 21
 3.865 31
 2.962 32
 3.961 27
 0.500 29
 3.178 26
 3.310 24
 3.538 30
 3.083 24
 3.013 24
 3.245 33
 2.963 27
 3.522 25
 3.013 31
 2.947 25
 2.118 20
 2.563 24
 3.357 21
 3.731 28
 3.925 27
 3.556 28
 3.101 26
 2.420 28
 2.579 22
 3.871 26
 3.060 21
 3.927 25
 2.375 16
 2.929 28
 3.375 26
 2.857 22
 3.072 24
 3.381 21
 3.290 30
 3.549 27
 3.646 26
 2.978 26
 2.654 30
 2.540 24
 2.250 26
 2.069 29
 2.617 24
 2.183 31
 2.000 15
 2.952 19
 3.806 18
 2.871 27
 3.352 16
 3.305 27
 2.952 26
 3.547 24
 3.691 30
 3.160 21
 2.194 20
 3.323 30
 3.936 29
 2.922 25
 2.716 23
 3.370 25
 3.606 23
 2.642 30
 2.452 21
 2.655 24
 3.714 32
 1.806 18
 3.516 23
 3.039 20
 2.966 23
 2.482 18
 2.700 18
 3.920 29
 2.834 20
 3.222 23
 3.084 26
 4.000 28
 3.511 34
 3.323 20
 3.072 20
 2.079 26
 3.875 32
 3.208 25
 2.920 27
 3.345 27
 3.956 29
 3.808 19
 2.506 21
 3.886 24
 2.183 27
 3.429 25
 3.024 18
 3.750 29
 3.833 24
 3.113 27
 2.875 21
 2.747 19
 2.311 18
 1.841 25
 1.583 18
 2.879 20
 3.591 32
 2.914 24
 3.716 35
 2.800 25
 3.621 28
 3.792 28
 2.867 25
 3.419 22
 3.600 30
 2.394 20
 2.286 20
 1.486 31
 3.885 20
 3.800 29
 3.914 28
 1.860 16
 2.948 28
", sep=" ")
options(scipen=999) # suppressing scientific notation

    
grade<-read.table(my.datafile,header=T)

attach(grade)


names(grade)
```

**(c)** Plot the residuals $e_{i}$ against the fitted values $\hat{Y}_{i}$. What departures from regression model (2.1) can be studied from this plot? What are your findings?

$$ Y_{i} = \beta_{0} + \beta_{1}X_{i} + \epsilon_{i} \ \ \ \ (2.1)$$

```{r GPA reg, echo=T, out.width='60%', fig.align='center'}
gpa.reg<-lm(GPA~ACT)
ggplot(data = data.frame(fitted(gpa.reg),resid(gpa.reg)), aes(x = fitted(gpa.reg), y = resid(gpa.reg))) + 
  geom_hline(yintercept = 0, color = "red") +
  geom_point(color='blue') +
  ggtitle("Residuals v.s Fitted Values Plot")
```

> One of the departures from the regression model we can investigate is whether the model is appropriate or not depending on if we see a pattern in the scatter plot of the residuals against the fitted values such as trending up or down in a linear or curved manner. Ideally, as in this plot, the plot should be appear to be random like a shotgun spread of points. The other departure from the regression model we can check here is whether the error terms have constant variance or not. We can see in the plot above, both conditions seem to be satisfied other than that there is a possibility of a couple of outliers in the error terms at the fitted values just above 3.2 and 3.3. 

\pagebreak

**(d)** Prepare a normal probability plot of the residuals. Also obtain the coeficient of correlation between the ordered residuals and their expected values under normality. Test the resonableness of the normality assumption here using Table B.6 and $\alpha =0.05$. What do you conclude?

```{r qqplot gpa, echo=T, out.width='60%', fig.align='center'}
qqplot.data <- function (vec) # argument: vector of numbers
{
  y <- quantile(vec[!is.na(vec)], c(0.25, 0.75))
  x <- qnorm(c(0.25, 0.75))
  slope <- diff(y)/diff(x)
  int <- y[1L] - slope * x[1L]
  d <- data.frame(resids = vec)
  ggplot(d, aes(sample = resids)) + stat_qq() + geom_abline(slope = slope, intercept = int, color = "red")
}
qqplot.data(resid(gpa.reg))+
  ggtitle("Normal Q-Q Plot of Residuals")
```

> In the normal Q-Q Plot above, we can see that the residuals are fairly normal as most of the points follow the Q-Q Line, but we can see departures at the tails which indicate the probability distribution has heavier tails. Formally we can test this with the Shapiro-Wilk Test for normality:

```{r shaprio-wilk gpa resid, include=T, echo= T}
shapiro.test(resid(gpa.reg))
```

> With a p-value less than 0.05, then the null hypothesis is rejected and there is evidence that the residuals are not normally distributed.

\pagebreak

### 3.6) Refer to Plastic Hardness Problem 1.27 from HW 3.

```{r plastic hardness data, include=F, echo=F}
my.datafile2 <- tempfile()
cat(file=my.datafile2, "
Hardness Time
 199.0  16.0
 205.0  16.0
 196.0  16.0
 200.0  16.0
 218.0  24.0
 220.0  24.0
 215.0  24.0
 223.0  24.0
 237.0  32.0
 234.0  32.0
 235.0  32.0
 230.0  32.0
 250.0  40.0
 248.0  40.0
 253.0  40.0
 246.0  40.0

", sep=" ")
options(scipen=999) # suppressing scientific notation

    
plastic<-read.table(my.datafile2,header=T)

attach(plastic)

names(plastic)

```

**(b)** Plot the residuals $e_{i}$ against the fitted values $\hat{Y}_{i}$ to ascertain whether any departures from the regression model (2.1) are evident. State your findings.

```{r Plastic reg, echo=T, out.width='60%', fig.align='center'}
plastic.reg<-lm(Hardness~Time)
ggplot(data = data.frame(fitted(plastic.reg),resid(plastic.reg)),
       aes(x = fitted(plastic.reg), y = resid(plastic.reg))) + 
  geom_hline(yintercept = 0, color = "red") +
  geom_point(color='blue') +
  ggtitle("Residuals v.s Fitted Values Plot")
```


> This plot does not seem to violate the assumptions of appropriateness of the model nor the constance variance of the error terms as there is no discernible pattern observed. 

\pagebreak

**(c)** Prepare a normal probability plot of the residuals. Also obtain the coeeficient of correlation between the ordered residuals and their expected values under normality. Does the normality assumption appear to be reasonable here? Use Table B.6 and $\alpha =0.05$.

```{r plastic qqplot, echo=T, fig.align='center', out.width='60%'}
qqplot.data(resid(plastic.reg))+
  ggtitle("Normal Q-Q Plot of Residuals")
```


>Here, there does not seem to be any significant departures from the Q-Q line and would indicate that the residuals are normally distributed. Again, we will formally test this with the Shapiro-Wilk Test for normality:

```{r plastic sw test, echo=T}
shapiro.test(resid(plastic.reg))
```

> With a p-value greater than 0.05, the null hypothesis is not rejected and there is evidence that the residuals are normally distributed.

\pagebreak

### 3.16) Refer to Solution Concentration Problem 3.15.

```{r solution data, include=F, echo= F}
my.datafile <- tempfile()
cat(file=my.datafile, "
Concentration Time
 0.07  9.0
 0.09  9.0
 0.08  9.0
 0.16  7.0
 0.17  7.0
 0.21  7.0
 0.49  5.0
 0.58  5.0
 0.53  5.0
 1.22  3.0
 1.15  3.0
 1.07  3.0
 2.84  1.0
 2.57  1.0
 3.10  1.0
", sep=" ")
options(scipen=999) # suppressing scientific notation

  
solution<-read.table(my.datafile,header=T)

attach(solution)

names(solution)

solution.reg<-lm(Concentration~Time)
```

**(a)** Prepare a scatterplot of the data. What transformation of $Y$ might you try, using the prototype patterns in Figure 3.15 to achieve constant variance and linearity?

```{r solution scatplot, echo=T, fig.align='center', out.width='60%'}
ggplot(data = solution,aes(x = Time, y = Concentration)) +
  geom_point(color='blue') +
  ggtitle("Solution Concentration Scatter Plot")
```

> According to Figure 3.15, with the downward curve trend of the scatterplot above, we may want to try a natural log transformation $Y\prime = log_{10} Y$ in order to achieve constant variance and linearity. Similarly, the Box Cox Transformation gives us a lambda value sufficiently close to 0 which would indicate the natural log transformation as the optimal transformation.

```{r solution box cox, echo=T, fig.align='center', out.width='60%'}
gg_boxcox(solution.reg)+
  ggtitle("Solution Regression Model Box Cox Transform Plot")
```

\pagebreak

**(c)** Use the tranformation $Y\prime = log_{10} Y$ and obtain the estimated linear regression function for the transformed data.

```{r transform}
cons_trans<- log10(Concentration)
trans_solution<- data.frame(cons_trans, Time)
trans_solution.reg<-lm(trans_solution$cons_trans~Time)
trans_solution.reg$coefficients
```

$$
\hat{Y}\prime = 0.6548798 - 0.1954003X
$$

**(d)** Plot the estimated regression line and the transformed data. Does the regression line appear to be a good fit to the transformed data?

```{r trans reg plot, echo = F, out.width='60%', fig.align='center'}
ggplot(data = trans_solution, aes(x = trans_solution$Time, 
                                  y = trans_solution$cons_trans)) + 
  geom_point(color='blue') +
  geom_smooth(method = "lm", se = T, color = "red") +
  ggtitle("Estimated Transformed Regression Function Plot")
```

> The transformed regression appears to be a great fit for the transformed data.

\pagebreak

**(e)** Obtain the residuals an plot them against the fitted values. Also prepare a normal probability plot. What do your plots show?

```{r, include = F, echo=F}
p_resid<- ggplot(data = data.frame(fitted(trans_solution.reg),resid(trans_solution.reg)),
       aes(x = fitted(trans_solution.reg), y = resid(trans_solution.reg))) + 
  geom_hline(yintercept = 0, color = "red") +
  geom_point(color='blue') +
  ggtitle("Residuals v.s Fitted Values Plot")

p_qq<- qqplot.data(resid(trans_solution.reg))+
  ggtitle("Normal Q-Q Plot of Residuals")
```

```{r solution trans resid q-q plots, include=T, echo=T, fig.align='center', out.width='80%'}
grid.arrange(p_resid,p_qq , ncol=2, nrow = 1)
```

> Both plots seem to to verify satisfaction of the assumption for constant veriance and normality. 

**(f)** Express the estimated regression function in the original units.

$$
\begin{aligned}
\hat{Y}\prime = log_{10}(\hat{Y}) &= 0.6548798 - 0.1954003X \\
\hat{Y} &= 10^{(0.6548798 - 0.1954003X)} \\
&= 10^{(0.6548798)} * 10^{(- 0.1954003)^{X}} \\
&= 4.517309 (0.6376755)^{X}
\end{aligned}
$$

\pagebreak

### 6.2) Set up the X matrix and $\beta$ vector for each of the following regression models (assume i = 1,...,5)

**(a)** $Y_{i} = \beta_{1}X_{i1} + \beta_{2}X_{i2} + \beta_{3}X_{i1}^{2} + \epsilon_{i}$


$$
\begin{bmatrix}
Y_1  \\ Y_2 \\ Y_3 \\ Y_4 \\ Y_5
\end{bmatrix}
=
\begin{bmatrix}
X_{11} & X_{12} & X_{11}^2 \\
X_{21} & X_{22} & X_{21}^2 \\
X_{31} & X_{32} & X_{31}^2 \\
X_{41} & X_{42} & X_{41}^2 \\
X_{51} & X_{52} & X_{51}^2 
\end{bmatrix}
\begin{bmatrix}
\beta_1 \\ \beta_3 \\ \beta_4 \\ \beta_5 
\end{bmatrix}
+
\begin{bmatrix}
\epsilon_1 \\ \epsilon_2 \\ \epsilon_3 \\ \epsilon_4 \\ \epsilon_5 
\end{bmatrix}
$$


**(b)** $\sqrt{Y_{i}} = \beta_{0} + \beta_{1}X_{i1} + \beta_{2}log_{10}X_{i2} + \epsilon_{i}$

$$
\begin{bmatrix}
\sqrt(Y_1)  \\ \sqrt(Y_2) \\ \sqrt(Y_3) \\ \sqrt(Y_4) \\ \sqrt(Y_5)
\end{bmatrix}
=
\begin{bmatrix}
1 & X_{11} & ln(X_{12})  \\
1 & X_{21} & ln(X_{22}) \\
1 & X_{31} & ln(X_{32})  \\
1 & X_{41} & ln(X_{42}) \\
1 & X_{51} & ln(X_{52})
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\ \beta_1 \\ \beta_3 \\ \beta_4 \\ \beta_5 
\end{bmatrix}
+
\begin{bmatrix}
\epsilon_1 \\ \epsilon_2 \\ \epsilon_3 \\ \epsilon_4 \\ \epsilon_5 
\end{bmatrix}
$$

\pagebreak

### 6.5) Brand Preference. In a small-scale expiremental study of the relation between degree of brand liking ($Y$) and moisture content ($X_{1}$) and sweetness ($X_{2}$) of the product, the following results were obtained from the experiment based on a completely randomized design. (data are coded)

```{r brand preference data, include=F, echo=F}
my.datafile <- tempfile()
cat(file=my.datafile, "
Liking Moisture Sweetness
 64.0  4.0  2.0
 73.0  4.0  4.0
 61.0  4.0  2.0
 76.0  4.0  4.0
 72.0  6.0  2.0
 80.0  6.0  4.0
 71.0  6.0  2.0
 83.0  6.0  4.0
 83.0  8.0  2.0
 89.0  8.0  4.0
 86.0  8.0  2.0
 93.0  8.0  4.0
 88.0 10.0  2.0
 95.0 10.0  4.0
 94.0 10.0  2.0
  100.0 10.0  4.0
", sep=" ")
options(scipen=999) # suppressing scientific notation

  
brand<-read.table(my.datafile,header=T)

attach(brand)

names(brand)
```

```{r brand kable, include = F, echo=T}
kable(head(brand), caption = "Brand Preference (Header)")
```

**(a)** Obtain the scatter plot matrix and the correlation matrix. What information do these diagnostic aids provide here?

```{r scatterplot and cor, include=T, echo=T, fig.align='center', out.width='60%'}
ggpairs(brand, aes(alpha = 0.4),
        upper = list(continuous = wrap("cor", size = 5, color = "red")))+
  ggtitle("Scatter Plot and Correlation Matrix")
```

> This plot gives us a lot of information such as the distributions of the variables, correlations between one another and the scatter plots of each variable. A couple to note here are that the distribution of 'Sweetness' looks to be bimodal, 'Moisture' looks to be fairly uniform, and 'Liking' looks to be slightly skewed left. Also we can see that 'Liking' is highly positively correlated with 'Moisture' and moderately positively correlated with 'Sweetness'. Additionally, we see that 'Moisture' and 'Sweetness' are not correlated at all. From this plot we can ascertain that the regression model satisfies the assumption for the relationships between the response ('Liking') and predictor variables ('Moisture' and 'Sweetness').

**(b)** Fit regression model (6.1) to the data. State the estimated regression function. How is $b_{1}$ interpreted here?

```{r brand reg, echo=T}
brand.reg<- lm(Liking~Moisture + Sweetness)
brand.reg$coefficients
```

$$
\hat{Y} = 37.65 + 4.425X_{1} + 4.375X_{2}
$$

> The interpretation of $b_{1}$ here is that it represents the difference in the predicted value of 'Liking' ($\hat{Y}$) for each one-unit difference in 'Moisture' ($X_{1}$), if 'Sweetness' ($X_{2}$) remains constant. So 'Liking' increases 4.425 (Whatever units 'Liking' is measured) per 1 'Moisture' unit while 'Sweetness' stays constant.

\pagebreak

**(d)** Plot the residuals against $\hat{Y}$, $X_{1}$, $X_{2}$, and $X_{1}X_{2}$ on separate graphs. Also prepare a normal probability plot. Interpret the plots and summarize your findings.

```{r, include=F, echo=F}
p1<- ggplot(data = data.frame(fitted(brand.reg),resid(brand.reg)),
       aes(x = fitted(brand.reg), y = resid(brand.reg))) + 
  geom_hline(yintercept = 0, color = "red") +
  geom_point(color='blue') +
  ggtitle("Residuals v.s Fitted Values Plot")

p2<- ggplot(data = data.frame(Moisture,resid(brand.reg)),
       aes(x = Moisture, y = resid(brand.reg))) + 
  geom_hline(yintercept = 0, color = "red") +
  geom_point(color='blue') +
  ggtitle("Residuals v.s Moisture Values Plot")

p3<- ggplot(data = data.frame(Sweetness,resid(brand.reg)),
       aes(x = Sweetness, y = resid(brand.reg))) + 
  geom_hline(yintercept = 0, color = "red") +
  geom_point(color='blue') +
  ggtitle("Residuals v.s Sweetness Values Plot")

s_M<- Sweetness*Moisture

p4<- ggplot(data = data.frame(s_M,resid(brand.reg)),
       aes(x = s_M, y = resid(brand.reg))) + 
  geom_hline(yintercept = 0, color = "red") +
  geom_point(color='blue') +
  ggtitle("Residuals v.s Sweet*Moist Values Plot")

```

```{r resid plots, echo=T, fig.align='center', out.width='60%'}
grid.arrange(p1, p2, p3, p4, ncol=2, nrow = 2)
```

> From the figure above, it appears there could be evidence of curvature in the residuals plotted against Moisture and Sweetness individually, and becomes more scattered in the product of Sweetness and Moisture. This could indicate that there is non-constant variance of the error terms or  non-appropriateness of the model. Although in the Q-Q Plot below, the residuals appear to satisfy the assumption of normality.

```{r resid norm qq, echo=T, fig.align='center', out.width='60%'}
qqplot.data(resid(brand.reg))+
  ggtitle("Normal Q-Q Plot of Residuals")
```

\pagebreak


### 6.6) Refer to Brand Preference problem 6.5. Assume that regression model (6.1) with independent normal error terms is appropriate.

**(a)** Test whether there is a regression relation, using $\alpha = 0.01$. State the alternatives, decision rule, and conclusion. What does your test imply about $\beta_{1}$ and $\beta_{2}$?

$$
H_{0}: \beta_{1} = \beta_{2} = 0 \ \ \ \ H_{1}: \beta_{1} \ \ or \ \ \beta_{2} \neq 0
$$

```{r reg relation, include=T,echo=T }
brand_reg_sum<- summary(brand.reg)
brand_reg_sum
brand_reg_sum$fstatistic[1] <= qf(1-.01, brand_reg_sum$df[1], brand_reg_sum$df[2] )
```

> Since $F^{*} > F_{(2,13)}$, we reject the null hypothesis and conclude that $\beta_{1} \ \ or \ \ \beta_{2} \neq 0$.

**(b)** What is the P-value of the test in part (a)?

> The p-value is given in the summary print out of the regression model with a p-value = 0.000000002658 which is a strong indicator that at least one of the Beta parameters are not 0.

### 6.7) Refer to Brand Preference problem 6.5. 

**(a)** Calculate the coefficients of multiple determination $R^{2}$. How is it interpreted here?

```{r coef mult deter, include=T, echo=T}
brand_reg_sum$adj.r.squared
```

> This is interpreted as 94.47% of the variation in 'Liking' is explained by the linear relationship of 'Moisture' and 'Sweetness', adjusted for multiple predictors. 

\pagebreak

### 6.8) Refer to Brand Preference problem 6.5. Assume that regression model (6.1) with independent normal error terms is appropriate.

**(a)** Obtain an interval estimate of $E[Y_{h}]$ when $X_{h1} = 5$ and $X_{h2} = 4$. Use a 99 percent confidence coefficient. Interpret you interval estimate.

```{r conf int, include=T, echo=T}
xh.values <- data.frame(cbind(Moisture = 5, Sweetness = 4))
predict(brand.reg, xh.values, interval="confidence", level=0.99)
```

> With 99% confidence the estimated true mean 'Likeness' of a brand with Moisture of 5 and Sweetness of 4 is between approximately 73.88 and 80.67.

**(b)** Obtain a prediction interval for a new observation $Y_{h(new)}$ when  $X_{h1} = 5$ and $X_{h2} = 4$. Use a 99 percent confidence coefficient.

```{r pred int, include=T, echo=T}
predict(brand.reg, xh.values, interval="prediction", level=0.99)
```

> With 99% probability the estimated true mean 'Likeness' of a brand with observed values Moisture of 5 and Sweetness of 4 is between approximately 68.48 and 86.07.

\pagebreak

### 6.18) Commercial Properties. A commercial real estate company evaluates vacancy rates, square footage, rental rates, and operating expenses for commercial properties in a large metropolitan area in order to provide clients with quantitative information upon which to make rental decisions. The data below are taken from 81 suburban commercial properties that are the newest, best located, most attractive, and expensive for five specific geographic areas. Shown here are the age ($X_{1}$), operating expenses and taxes ($X_{2}$), vacancy rates ($X_{3}$), total square footage ($X_{4}$), and rental rates ($Y$).

```{r commercial data, include=F, echo=F}
my.datafile <- tempfile()
cat(file=my.datafile, "
Rental Age Operating Vacancy Total 
 13.500 1 5.02 0.14 123000
 12.000 14 8.19 0.27 104079
 10.500 16 3.00 0.00 39998
 15.000 4 10.70 0.05 57112
 14.000 11 8.97 0.07 60000
 10.500 15 9.45 0.24 101385
 14.000 2 8.00 0.19 31300
 16.500 1 6.62 0.60 248172
 17.500 1 6.20 0.00 215000
 16.500 8 11.78 0.03 251015
 17.000 12 14.62 0.08 291264
 16.500 2 11.55 0.03 207549
 16.000 2 9.63 0.00 82000
 16.500 13 12.99 0.04 359665
 17.225 2 12.01 0.03 265500
 17.000 1 12.01 0.00 299000
 16.000 1 7.99 0.14 189258
 14.625 12 10.33 0.12 366013
 14.500 16 10.67 0.00 349930
 14.500 3 9.45 0.03 85335
 16.500 6 12.65 0.13 235932
 16.500 3 12.08 0.00 130000
 15.000 3 10.52 0.05 40500
 15.000 3 9.47 0.00 40500
 13.000 14 11.62 0.00 45959
 12.500 1 5.00 0.33 120000
 14.000 15 9.89 0.05 81243
 13.750 16 11.13 0.06 153947
 14.000 2 7.96 0.22 97321
 15.000 16 10.73 0.09 276099
 13.750 2 7.95 0.00 90000
 15.625 3 9.10 0.00 184000
 15.625 3 12.05 0.03 184718
 13.000 16 8.43 0.04 96000
 14.000 16 10.60 0.04 106350
 15.250 13 10.55 0.10 135512
 16.250 1 5.50 0.21 180000
 13.000 14 8.53 0.03 315000
 14.500 3 9.04 0.04 42500
 11.500 15 8.20 0.00 30005
 14.250 1 6.13 0.00 60000
 15.500 15 8.32 0.00 73521
 12.000 1 4.00 0.00 50000
 14.250 15 10.10 0.00 50724
 14.000 3 5.25 0.16 31750
 16.500 3 11.62 0.00 168000
 14.500 4 5.31 0.00 70000
 15.500 1 5.75 0.00 27000
 16.750 4 12.46 0.03 129614
 16.750 4 12.75 0.00 129614
 16.750 2 12.75 0.00 130000
 16.750 2 11.38 0.00 209000
 17.000 1 5.99 0.57 220000
 16.000 2 11.37 0.27 60000
 14.500 3 10.38 0.00 110000
 15.000 15 10.77 0.05 101206
 15.000 17 11.30 0.00 288847
 16.000 1 7.06 0.14 105000
 15.500 14 12.10 0.05 276425
 15.250 2 10.04 0.06 33000
 16.500 1 4.99 0.73 210000
 19.250 0 7.33 0.22 240000
 17.750 18 12.11 0.00 281552
 18.750 16 12.86 0.00 421000
 19.250 13 12.70 0.04 484290
 14.000 20 11.58 0.00 234493
 14.000 18 11.58 0.03 230675
 18.000 16 12.97 0.08 296966
 13.750 1 4.82 0.00 32000
 15.000 2 9.75 0.03 38533
 15.500 16 10.36 0.02 109912
 15.900 1 8.13 0.23 236000
 15.250 15 13.23 0.05 243338
 15.500 4 10.57 0.04 122183
 14.750 20 11.22 0.00 128268
 15.000 3 10.34 0.00 72000
 14.500 3 10.67 0.00 43404
 13.500 18 8.60 0.08 59443
 15.000 15 11.97 0.14 254700
 15.250 11 11.27 0.03 434746
 14.500 14 12.68 0.03 201930
", sep=" ")
options(scipen=999) # suppressing scientific notation

  
commercial<-read.table(my.datafile,header=T)

attach(commercial)

names(commercial)
```


```{r comm kable, echo=F }
kable(head(commercial), caption = "Commercial Properties (Header)")
```

**(c)** Fit regression model (6.5) for four predictor variables to the data. State the estimated regression function.

```{r comm reg model, include=T, echo=T}
comm.reg<- lm(Rental~Age+Operating+Vacancy+Total)
comm.reg$coefficients
```

$$
\hat{Y} = 12.201 - 0.142X_{1} + 0.282X_{2} + 0.619X_{3} + 0.000008X_{4}
$$

**(e)** Plot the residuals against $\hat{Y}$, each predictor variable, and each two-factor interaction term on separate graphs. Also prepare a normal probability plot. Analyze your plots and summarize your findings. 

```{r comm resid plots, include=F,echo=F}
com_yh<-ggplot(data = data.frame(fitted(comm.reg),resid(comm.reg)),
       aes(x = fitted(comm.reg), y = resid(comm.reg))) + 
  geom_hline(yintercept = 0, color = "red") +
  geom_point(color='blue') +
  ggtitle("Fitted Values Plot")

com_x1<-ggplot(data = data.frame(Age,resid(comm.reg)),
       aes(x = Age, y = resid(comm.reg))) + 
  geom_hline(yintercept = 0, color = "red") +
  geom_point(color='blue') +
  ggtitle("Age Plot")

com_x2<-ggplot(data = data.frame(Operating,resid(comm.reg)),
       aes(x = Operating, y = resid(comm.reg))) + 
  geom_hline(yintercept = 0, color = "red") +
  geom_point(color='blue') +
  ggtitle("Operating Plot")

com_x3<-ggplot(data = data.frame(Vacancy,resid(comm.reg)),
       aes(x = Vacancy, y = resid(comm.reg))) + 
  geom_hline(yintercept = 0, color = "red") +
  geom_point(color='blue') +
  ggtitle("Vacancy Plot")

com_x4<-ggplot(data = data.frame(Total,resid(comm.reg)),
       aes(x = Total, y = resid(comm.reg))) + 
  geom_hline(yintercept = 0, color = "red") +
  geom_point(color='blue') +
  ggtitle("Total Plot")

com_x1x2<-ggplot(data = data.frame(Age*Operating,resid(comm.reg)),
       aes(x = Age*Operating, y = resid(comm.reg))) + 
  geom_hline(yintercept = 0, color = "red") +
  geom_point(color='blue') +
  ggtitle("Age*Operating Plot")

com_x1x3<-ggplot(data = data.frame(Age*Vacancy,resid(comm.reg)),
       aes(x = Age*Vacancy, y = resid(comm.reg))) + 
  geom_hline(yintercept = 0, color = "red") +
  geom_point(color='blue') +
  ggtitle("Age*Vacancy Plot")

com_x1x4<-ggplot(data = data.frame(Age*Total,resid(comm.reg)),
       aes(x = Age*Total, y = resid(comm.reg))) + 
  geom_hline(yintercept = 0, color = "red") +
  geom_point(color='blue') +
  ggtitle("Age*Total Plot")

com_x2x3<-ggplot(data = data.frame(Operating*Vacancy,resid(comm.reg)),
       aes(x = Operating*Vacancy, y = resid(comm.reg))) + 
  geom_hline(yintercept = 0, color = "red") +
  geom_point(color='blue') +
  ggtitle("Operating*Vacancy Plot")

com_x2x4<-ggplot(data = data.frame(Operating*Total,resid(comm.reg)),
       aes(x = Operating*Total, y = resid(comm.reg))) + 
  geom_hline(yintercept = 0, color = "red") +
  geom_point(color='blue') +
  ggtitle("Operating*Total Plot")

com_x3x4<-ggplot(data = data.frame(Vacancy*Total,resid(comm.reg)),
       aes(x = Vacancy*Total, y = resid(comm.reg))) + 
  geom_hline(yintercept = 0, color = "red") +
  geom_point(color='blue') +
  ggtitle("Vacancy*Total Plot")

com_qq<- qqplot.data(resid(comm.reg))+
  ggtitle("Normal Q-Q Plot of Residuals")
```

```{r comm grid plots, include=T, echo=T, fig.align='center'}
grid.arrange(com_yh,com_x1,com_x2,com_x3,com_x4,com_qq, ncol=3, nrow = 2)
```

\pagebreak

> One of the first things that catches my eye is the Q-Q Plot, the departure from the q-q line may indicate heavy tails and non-normality of the residuals, but wth 81 observations, the Central Limit Theorem may kick in for us. Under the formal test we in fact do achieve normality. Another aspect of the plots that sticks out are the plots where the 'Vacancy' predictor variable are involved have heavy skewness because there are 30 out of the 81 observations that are 0. This could have implications of whether this predictor variable is actually useful or if the model could be improved by removing it.


```{r more comm grid plots, include=T, echo=T, fig.align='center'}
grid.arrange(com_x1x2,com_x1x3,com_x1x4,com_x2x3,com_x2x4, com_x3x4, ncol=3, nrow = 2)
```

\pagebreak

### 6.19) Refer to Commercial Properties problem 6.18. Assume that regression model (6.5) for four predictor variables with independent normal error terms is appropriate. 

**(a)** Test whether there is a regression relation; use $\alpha = 0.05$. State the alternatives, decision rule, and conclusion. What does your test imply about $\beta_{1}$, $\beta_{2}$, $\beta_{3}$, and $\beta_{4}$? What is the P-value of the test?

$$
H_{0}: \beta_{1} = \beta_{2} = \beta_{3} = \beta_{4} = 0 \ \ \ \ H_{1}: \beta_{k} \neq 0 \ \ for \ \ some \ \ k = 1,..., 4
$$

```{r comm reg F test, include=T, echo=T}
comm_reg_sum<- summary(comm.reg)
comm_reg_sum
comm_reg_sum$fstatistic[1] <= qf(1-.05, comm_reg_sum$df[1], comm_reg_sum$df[2] )
```

> Since $F^{*} > F_{(4,76)}$, we reject the null hypothesis and conclude that $H_{1}: \beta_{k} \neq 0 \ \ for \ \ some \ \ k = 1,..., 4$. However, looking at the p-value associated with 'Vacancy' which is $> \alpha = 0.05$ suggesting that this predictor may not have an impact on our model. The flip side of the same coin is that low p-values don't necessarily identify predictor variables that are practically important.


**(c)** Calculate $R^{2}$ and interpret this measure.

```{r adj r square, include=T, echo=T}
comm_reg_sum$adj.r.squared
```

> With $R^{2} = 0.5629$, we can conclude that 56.29% of the variation in Rental Rates is explained by the linear relationship with the predictor variables Age, Operating Expenses, Vacancy Rate, and Total Square Footage. On a side note, when the model is estimated without the Vacancy variable, $R^{2} = 0.583$, which is slightly higher than the full model. With an $R^{2}$ value this low and the non-significant difference in variance explained between the two models, we may have evidence to suggest that the model is not appropriate.

\pagebreak

### 6.22) For each of the following regression models, indicate whether it is a general linear regression model. If it is not, state whether it can be expressed in the form of (6.7) by a suitbable transformation.

**(a)** $Y_{i} = \beta_{0} + \beta_{1}X_{i1} + \beta_{2}log_{10}X_{i2} + \beta_{3}X_{i1}^{2} + \epsilon_{i}$

> No, but a transformation of this regression model is a general linear regression model because it is linear in the parameters and can be transformed as the following:

Let $Z_{i1} = X_{i1} , \ Z_{i2} = log_{10}X_{i2} , \ Z_{i3} = X_{i1}^{2}$

Then $Y_{i} = \beta_{0} + \beta_{1}Z_{i1} + \beta_{2}Z_{i2} + \beta_{3}Z_{i3} + \epsilon_{i}$


**(b)** $Y_{i} = log_{10}(\beta_{1}X_{i1}) + \beta_{2}X_{i2} + \epsilon_{i}$

> No, this regression model is not linear in the parameters and cannot be put into general form by a transformation.

\pagebreak

### 6.23) Consider the following miltiple regression model:

$$
Y_{i} = \beta_{1}X_{i1} + \beta_{2}X_{i2} + \epsilon_{i} \ \ \ i = 1,...,n
$$

$$
where \ \ \epsilon_{i} \ \ \ are \ \ \ uncorrelated, \ \ \ E[\epsilon_{i}] = 0  \ \ \ and \ \ \ \sigma^{2}\lbrace\epsilon_{i}\rbrace = \sigma^{2}
$$

**(a)** State the least squares criterion and derive the least squares estimators of $\beta_{1}$ and $\beta_{2}$.


